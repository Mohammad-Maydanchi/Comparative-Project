{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6674905,"sourceType":"datasetVersion","datasetId":1936563}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center> \n<img src = \"https://www.elastic.co/guide/en/machine-learning/master/images/classification-vis.png\" width = 800 height = 400/>\n</center>\n<br>\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a collaborative project between all of the states in the United States and participating US territories and the Centers for Disease Control and Prevention (CDC). The BRFSS is a system of ongoing health-related telephone surveys designed to collect data on health-related risk behaviors, chronic health conditions, and the use of preventive services from the non-institutionalized adult population (≥ 18 years) residing in the United States. The BRFSS is administered and supported by CDC's Population Health Surveillance Branch, under the Division of Population Health at CDC's National Center for Chronic Disease Prevention and Health Promotion.\n\nOriginally, the dataset come from the CDC (1) and is a major part of the Behavioral Risk Factor Surveillance System (BRFSS), which conducts annual telephone surveys to gather data on the health status of U.S. residents. As the CDC describes: \"Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories.  BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\". The most recent dataset (as of February 15, 2022) includes data from 2020. It consists of 401,958 rows and 279 columns. \n\nThe vast majority of columns are questions asked to respondents about their health status, such as \"Do you have serious difficulty walking or climbing stairs?\" or \"Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]\". In this dataset, We noticed many different factors (questions) that directly or indirectly influence heart disease, so we decided to select the most relevant variables from it and do some cleaning so that it would be usable for machine learning projects (2).\n\n1. [codebook20_llcp-v2-508.pdf](https://www.cdc.gov/brfss/annual_data/2020/pdf/codebook20_llcp-v2-508.pdf)\n2. [personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)\n\n\n<hr>","metadata":{"id":"Y_kU-rIg7mvi","papermill":{"duration":0.027642,"end_time":"2023-05-13T04:39:55.577743","exception":false,"start_time":"2023-05-13T04:39:55.550101","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Business Understanding\nHeart disease is the leading cause of death in the United States. The term \"heart disease\" refers to several types of heart conditions. The most common type of heart disease in the United States is coronary artery disease (CAD), which can lead to a heart attack. Machine learning leads to a better understanding of how we can predict heart disease.","metadata":{"papermill":{"duration":0.027377,"end_time":"2023-05-13T04:39:55.631755","exception":false,"start_time":"2023-05-13T04:39:55.604378","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n\n<a id=\"title-two\"></a>\n# 2. Data Collecting\n### 2.1 About the dataset\n\nThe Behavioral Risk Factor Surveillance System (BRFSS) is a collaborative project between all of the states in the United States and participating US territories and the Centers for Disease Control and Prevention (CDC). The BRFSS is a system of ongoing health-related telephone surveys designed to collect data on health-related risk behaviors, chronic health conditions, and the use of preventive services from the non-institutionalized adult population (≥ 18 years) residing in the United States. The BRFSS is administered and supported by CDC's Population Health Surveillance Branch, under the Division of Population Health at CDC's National Center for Chronic Disease Prevention and Health Promotion. \n\nFactors assessed by the BRFSS in 2020 included health status and healthy days, exercise, inadequate sleep, chronic health conditions, oral health, tobacco use, cancer screenings, and health-care access (core section). Optional Module topics for 2020 included prediabetes and diabetes, cognitive decline, electronic cigarettes, cancer survivorship (type, treatment, pain management) and sexual orientation/gender identity (SOGI).\n\nOriginally, the dataset come from the CDC and is a major part of the Behavioral Risk Factor Surveillance System (BRFSS), which conducts annual telephone surveys to gather data on the health status of U.S. residents. As the CDC describes: \"Established in 1984 with 15 states, BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400,000 adult interviews each year, making it the largest continuously conducted health survey system in the world.\". The most recent dataset (as of February 15, 2022) includes data from 2020. It consists of 401,958 rows and 279 columns. The vast majority of columns are questions asked to respondents about their health status, such as \"Do you have serious difficulty walking or climbing stairs?\" or \"Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]\". In this dataset, We noticed many different factors (questions) that directly or indirectly influence heart disease, so we decided to select the most relevant variables from it and do some cleaning so that it would be usable for machine learning projects ([1](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)).\n\n<br>\n\n### 2.2 Dataset Description [source](https://www.cdc.gov/brfss/annual_data/2020/pdf/codebook20_llcp-v2-508.pdf)\n\n|Category|Label|Question|Value| \n|-|-|-|-|\n|<b>HeartDisease</b>|Ever had CHD or MI| <i>-Respondents that have ever reported having coronary <br> -heart disease (CHD) or myocardial infarction (MI)</i>|-Yes<br>-No|\n|<b>BMI</b>|Computed body mass index|<i>Computed body mass index</i>|Float[1-9999]\n|<b>Smoking</b>|Smoked at Least 100 Cigarettes|<i>Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]</i>|-Yes<br>-No|\n|<b>AlcoholDrinking</b>|Heavy Alcohol Consumption Calculated Variable|<i>Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)</i>|-Yes<br>-No|\n|<b>Stroke</b>|Ever Diagnosed with a Stroke|<i>(Ever told) (you had) a stroke.</i>|-Yes<br>-No|\n|<b>PhysicalHealth</b>|Number of Days Physical Health Not Good|<i>Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?</i>|Number of days [1-30]|\n|<b>MentalHealth</b>|Number of Days Mental Health Not Good|<i>Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?</i>|Number of days [1-30]|\n|<b>DiffWalking</b>|Difficulty Walking or Climbing Stairs|<i>Do you have serious difficulty walking or climbing stairs?</i>|-Yes<br>-No|\n|<b>Sex</b>|Are you male or female?|<i>Are you male or female?</i>|-Male<br>-Female|\n|<b>AgeCategory</b>|Reported age in five-year age categories calculated variable|<i>Fourteen-level age category</i>|-Age [18-79]<br>-Age [80 or older]|\n|<b>Race</b>|Imputed race/ethnicity value|<i>Imputed race/ethnicity value (This value is the reported race/ethnicity or an imputed race/ethnicity, if the respondent refused to give a race/ethnicity. The value of the imputed race/ethnicity will be the most common race/ethnicity response for that region of the state)</i>|-White<br>-Black<br>-Asian<br>-American Indian/Alaskan Native<br>-Hispanic<br>-Other|\n|<b>Diabetic</b>|(Ever told) you had diabetes|<i>(Ever told) (you had) diabetes? (If ´Yes´ and respondent is female, ask ´Was this only when you were pregnant?´. If Respondent says pre-diabetes or borderline diabetes, use response code 4.)</i>|-Yes<br>-No<br>-No, borderline diabetes<br>-Yes (during pregnancy)|\n|<b>PhysicalActivity</b>|Exercise in Past 30 Days|<i>During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise?</i>|-Yes<br>-No|\n|<b>GenHealth</b>|General Health|<i>Would you say that in general your health is:</i>|-Excellent<br>-Very good<br>-Good<br>-Fair<br>-Poor|\n|<b>SleepTime</b>|How Much Time Do You Sleep|<i>On average, how many hours of sleep do you get in a 24-hour period?</i>|Number of hours [1-24]|\n|<b>Asthma</b>|Ever Told Had Asthma|<i>(Ever told) (you had) asthma?</i>|-Yes<br>-No|\n|<b>KidneyDisease</b>|Ever told you have kidney disease?|<i>Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?</i>|-Yes<br>-No|\n|<b>SkinCancer</b>|(Ever told) you had skin cancer?|<i>(Ever told) (you had) skin cancer?</i>|-Yes<br>-No|","metadata":{"papermill":{"duration":0.024856,"end_time":"2023-05-13T04:39:55.682045","exception":false,"start_time":"2023-05-13T04:39:55.657189","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import pandas as pd for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('/kaggle/input/personal-key-indicators-of-heart-disease/heart_2020_cleaned.csv')\ndf_org = df.copy() # make a copy of the original data frame\ndisplay(df)","metadata":{"papermill":{"duration":2.001766,"end_time":"2023-05-13T04:39:57.708373","exception":false,"start_time":"2023-05-13T04:39:55.706607","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-05-21T07:02:45.46817Z","iopub.execute_input":"2023-05-21T07:02:45.46933Z","iopub.status.idle":"2023-05-21T07:02:46.188446Z","shell.execute_reply.started":"2023-05-21T07:02:45.469276Z","shell.execute_reply":"2023-05-21T07:02:46.187431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<hr>\n\n<a id=\"title-three\"></a>\n# 3. Data Understanding\n### 3.1 Listing the unique values of each column\nPython is a great language for doing data analysis, primarily because of the fantastic ecosystem of data-centric Python packages. Pandas is one of those packages, and makes importing and analyzing data much easier.\n\nWhile analyzing the data, many times the user wants to see the unique values in a particular column, which can be done using Pandas unique() function.","metadata":{"papermill":{"duration":0.026623,"end_time":"2023-05-13T04:39:57.761137","exception":false,"start_time":"2023-05-13T04:39:57.734514","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# retrieve all labels and store in a list\ncolumns_df = list(df.columns.values)\n# iterate over the list to print all unique values of each column in the dataframe\nfor column in columns_df:\n    print(column, ':', str(df[column].unique()))","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:57.81816Z","iopub.status.busy":"2023-05-13T04:39:57.817519Z","iopub.status.idle":"2023-05-13T04:39:58.259173Z","shell.execute_reply":"2023-05-13T04:39:58.256939Z"},"papermill":{"duration":0.474774,"end_time":"2023-05-13T04:39:58.262938","exception":false,"start_time":"2023-05-13T04:39:57.788164","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Continuous vs Categorical\nEvery dataset has two type of variables Continuous(Numerical) and Categorical. Regression based algorithms use continuous and categorical features to build the models. You can’t fit categorical variables into a regression equation in their raw form in most of the ML Libraries.\n\n### need to move after descriptive statistics\n#### 3.2.1 Return a subset of the DataFrame’s columns based on the column dtypes\n`DataFrame.select_dtypes(include=None, exclude=None)`\n- To select all numeric types, use `np.number` or `number`.\n- To select strings you must use the `object` dtype, but note that this will return all object dtype columns.","metadata":{"papermill":{"duration":0.035117,"end_time":"2023-05-13T04:39:58.337246","exception":false,"start_time":"2023-05-13T04:39:58.302129","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import numpy for array operations and select all numerical columns\nimport numpy as np","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:58.393326Z","iopub.status.busy":"2023-05-13T04:39:58.392783Z","iopub.status.idle":"2023-05-13T04:39:58.399956Z","shell.execute_reply":"2023-05-13T04:39:58.398326Z"},"papermill":{"duration":0.03851,"end_time":"2023-05-13T04:39:58.403315","exception":false,"start_time":"2023-05-13T04:39:58.364805","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list of numerical features\nnumeric_features = df.select_dtypes(include=[np.number])\nnumeric_features.columns","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:58.45684Z","iopub.status.busy":"2023-05-13T04:39:58.456315Z","iopub.status.idle":"2023-05-13T04:39:58.474083Z","shell.execute_reply":"2023-05-13T04:39:58.472839Z"},"papermill":{"duration":0.048033,"end_time":"2023-05-13T04:39:58.476976","exception":false,"start_time":"2023-05-13T04:39:58.428943","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# list of categorical features\ncategorical_features = df.select_dtypes(include=[object])\ncategorical_features.columns","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:58.533276Z","iopub.status.busy":"2023-05-13T04:39:58.53177Z","iopub.status.idle":"2023-05-13T04:39:58.599964Z","shell.execute_reply":"2023-05-13T04:39:58.598485Z"},"papermill":{"duration":0.099527,"end_time":"2023-05-13T04:39:58.60295","exception":false,"start_time":"2023-05-13T04:39:58.503423","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3 Generate descriptive statistics\nDescriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values.\n- `DataFrame.describe(percentiles=None, include=None, exclude=None, datetime_is_numeric=False)`\n\nBefore generating statistics on our dataset, if you look at the 'AgeCategory' column, you notice that unique values are in the range of [a-b] where a is minimum, and b is maximum.\n- AgeCategory : [ '18-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-64', '65-69', '70-74', '75-79', '80 or older']\n\nWe have to encode the column and do some calculations so that it would be usable for generating statistics. \n- One of the easiest methods to convert [a-b] to an integer is to calculate the mean and replace it with the original value. \n- For example, if we have [55-59], the mean equals 57. Hence, all values from 55 to 59 will replace with 57.","metadata":{"papermill":{"duration":0.02705,"end_time":"2023-05-13T04:39:58.656328","exception":false,"start_time":"2023-05-13T04:39:58.629278","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# encode 'AgeCategory' column\nencode_AgeCategory = {'55-59':57, '80 or older':80, '65-69':67,\n                      '75-79':77,'40-44':42,'70-74':72,'60-64':62,\n                      '50-54':52,'45-49':47,'18-24':21,'35-39':37,\n                      '30-34':32,'25-29':27}\ndf['AgeCategory'] = df['AgeCategory'].apply(lambda x: encode_AgeCategory[x])\ndf['AgeCategory'] = df['AgeCategory'].astype(int)\ndf['AgeCategory']","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:58.712535Z","iopub.status.busy":"2023-05-13T04:39:58.711986Z","iopub.status.idle":"2023-05-13T04:39:59.04145Z","shell.execute_reply":"2023-05-13T04:39:59.03998Z"},"papermill":{"duration":0.361237,"end_time":"2023-05-13T04:39:59.044597","exception":false,"start_time":"2023-05-13T04:39:58.68336","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate descriptive statistics\ndf.describe()[1:][list(numeric_features)].T.style.background_gradient(cmap='Blues')","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:59.101736Z","iopub.status.busy":"2023-05-13T04:39:59.101172Z","iopub.status.idle":"2023-05-13T04:39:59.318645Z","shell.execute_reply":"2023-05-13T04:39:59.317122Z"},"papermill":{"duration":0.249896,"end_time":"2023-05-13T04:39:59.321659","exception":false,"start_time":"2023-05-13T04:39:59.071763","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<hr>\n\n<a id=\"title-four\"></a>\n# 4. Exploratory Data Analysis\nExploratory Data Analysis(EDA) is an approach to analyse the data , to summarize its characteristics , often with visual methods.\n\nExploratory Data Analysis is majorly performed using the following methods:\n- Univariate visualization — provides summary statistics for each field in the raw data set.\n- Bivariate visualization — is performed to find the relationship between each variable in the dataset and the target variable of interest.\n\n\n### 4.1 Univariate visualization\n#### 4.1.1 Univariate visualization of categorical features\nprovides summary statistics for each categorical field in the raw data set.","metadata":{"papermill":{"duration":0.027586,"end_time":"2023-05-13T04:39:59.377208","exception":false,"start_time":"2023-05-13T04:39:59.349622","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import matplotlib and seaborn for visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Univariate visualization of categorical features\ndef categorical_feature_func():\n  i = 1\n  plt.figure(figsize = (25,15))\n  for feature in categorical_features:\n      plt.subplot(3,5,i)\n      sns.set(palette='Paired')\n      sns.set_style(\"ticks\")\n      ax = sns.countplot(x = feature, data = df)#, hue = 'Stroke')#, color='#221C35') \n      ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n      i +=1\n\ncategorical_feature_func()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:39:59.435418Z","iopub.status.busy":"2023-05-13T04:39:59.434418Z","iopub.status.idle":"2023-05-13T04:40:08.400473Z","shell.execute_reply":"2023-05-13T04:40:08.398769Z"},"papermill":{"duration":8.99861,"end_time":"2023-05-13T04:40:08.404092","exception":false,"start_time":"2023-05-13T04:39:59.405482","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.1.2 Univariate visualization of numerical features\nprovides summary statistics for each numerical field in the raw data set.","metadata":{"papermill":{"duration":0.030321,"end_time":"2023-05-13T04:40:08.467698","exception":false,"start_time":"2023-05-13T04:40:08.437377","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Univariate visualization of numerical features\ndef numeric_features_func():\n  i=1\n  plt.figure(figsize = (35,5))\n  for feature in numeric_features.columns:\n      plt.subplot(1,5,i)\n      sns.set(palette='dark')\n      sns.set_style(\"ticks\")\n      sns.histplot(df[feature],kde=True)\n      plt.xlabel(feature)\n      plt.ylabel(\"Count\")\n      i+=1\n\nnumeric_features_func()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:08.532446Z","iopub.status.busy":"2023-05-13T04:40:08.531835Z","iopub.status.idle":"2023-05-13T04:40:20.655828Z","shell.execute_reply":"2023-05-13T04:40:20.653831Z"},"papermill":{"duration":12.160231,"end_time":"2023-05-13T04:40:20.659266","exception":false,"start_time":"2023-05-13T04:40:08.499035","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 Bivariate visualization\n#### 4.2.1 Bivariate visualization of categorical features\nrelationship between each categorical variable in the dataset and the target (HeartDisease) variable of interest.","metadata":{"papermill":{"duration":0.03266,"end_time":"2023-05-13T04:40:20.725526","exception":false,"start_time":"2023-05-13T04:40:20.692866","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def categorical_feature_func():\n  i = 1\n  plt.figure(figsize = (25,15))\n  for feature in categorical_features:\n      plt.subplot(3,5,i)\n      sns.set(palette='Paired')\n      sns.set_style(\"ticks\")\n      ax = sns.countplot(x = feature, data = df, hue = 'HeartDisease')\n      ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n      i +=1\n\ncategorical_feature_func()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:20.793352Z","iopub.status.busy":"2023-05-13T04:40:20.792779Z","iopub.status.idle":"2023-05-13T04:40:33.19249Z","shell.execute_reply":"2023-05-13T04:40:33.190539Z"},"papermill":{"duration":12.439882,"end_time":"2023-05-13T04:40:33.198067","exception":false,"start_time":"2023-05-13T04:40:20.758185","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 4.2.2 Bivariate visualization of numerical features\nrelationship between each numerical variable in the dataset and the target (DiffWalking) variable of interest.","metadata":{"papermill":{"duration":0.038022,"end_time":"2023-05-13T04:40:33.274087","exception":false,"start_time":"2023-05-13T04:40:33.236065","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def numeric_features_func(f):\n  i=1\n  plt.figure(figsize=(35,5))\n  sns.set(palette='Paired')\n  sns.set_style(\"ticks\")\n  for feature in numeric_features:\n      plt.subplot(1,5,i)\n      sns.boxplot(y=df[feature], x = df[f])\n      i+=1\n\nnumeric_features_func('DiffWalking')","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:33.353458Z","iopub.status.busy":"2023-05-13T04:40:33.35218Z","iopub.status.idle":"2023-05-13T04:40:35.744238Z","shell.execute_reply":"2023-05-13T04:40:35.742688Z"},"papermill":{"duration":2.435191,"end_time":"2023-05-13T04:40:35.747527","exception":false,"start_time":"2023-05-13T04:40:33.312336","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<hr>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"title-five\"></a>\n# 5. Preprocessing\n\n### 5.1 Label Encoding\nIn machine learning, encoding is a process of converting categorical data into a form that can be easily understood by machine learning algorithms. As many machine learning models only accept numerical inputs, encoding is a crucial step in preparing your data for model training. There are multiple types of encoding, each with its own use cases:\n\n- Label Encoding: This involves converting each value in a column to a number. For example, 'red' could be 1, 'green' could be 2, and so on. While this is a straightforward method, it can sometimes lead to problems as it may introduce an arbitrary ordering where none exists. For example, the model might incorrectly learn that 'green' > 'red' because 2 > 1, which is not necessarily a valid comparison.\n\n- One-Hot Encoding: This involves creating a new binary column for each category in the data. For instance, for a column 'Color' with categories 'red', 'green', and 'blue', one-hot encoding would create three new columns, 'Color_red', 'Color_green', and 'Color_blue', which are either 0 or 1 depending on the color of the instance. This method can lead to a large increase in the dataset's dimensionality if a categorical variable has many categories, potentially slowing down training and reducing performance.\n\n- Mixed Encoding: Sometimes, a combination of the above methods can be used based on the cardinality of the categorical variables. For example, label encoding could be applied to binary variables (with only two categories), and one-hot encoding could be applied to non-binary variables. This approach aims to balance the advantages and drawbacks of both encoding methods.\n\n\n<br>\nExample: \n\n- Original: `['GenHealth'] : ['Very good' 'Fair' 'Good' 'Poor' 'Excellent']`\n- Encoded: \n  - `['GenHealth_Very good'] = [1, 0]`\n  - `['GenHealth_Fair'] = [0, 1]`\n  - `['GenHealth_Good'] = [0, 0]`\n  - `['GenHealth_Poor'] = [0, 0]`\n  - `['GenHealth_Excellent'] = [0, 0]`","metadata":{"papermill":{"duration":0.038738,"end_time":"2023-05-13T04:40:35.826344","exception":false,"start_time":"2023-05-13T04:40:35.787606","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Encode all columns\ncolumns_df = list(df.columns.values)\nfrom sklearn.preprocessing import LabelEncoder\n\ncat_cols = [\"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\",\n                \"Sex\", \"AgeCategory\", \"Race\", \"Diabetic\", \"PhysicalActivity\",\n                \"GenHealth\", \"Asthma\", \"KidneyDisease\", \"SkinCancer\"]\nfor cat_col in cat_cols:\n    dummy_col = pd.get_dummies(df[cat_col], prefix=cat_col)\n    df = pd.concat([df, dummy_col], axis=1)\n    del df[cat_col]\n\nfor col in ['HeartDisease']:\n    if df[col].dtype == 'O':\n        le = LabelEncoder()\n        df[col] = le.fit_transform(df[col])\n\n\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:35.906584Z","iopub.status.busy":"2023-05-13T04:40:35.905771Z","iopub.status.idle":"2023-05-13T04:40:37.228103Z","shell.execute_reply":"2023-05-13T04:40:37.226539Z"},"papermill":{"duration":1.368512,"end_time":"2023-05-13T04:40:37.233067","exception":false,"start_time":"2023-05-13T04:40:35.864555","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<b> UPDATE </b>\nWe added 1 more encoding method:\n- Original:\n  - OneHotEncoder\n    - \"Smoking\", \"AlcoholDrinking\", \"Stroke\", \"DiffWalking\",\n    - \"Sex\", \"AgeCategory\", \"Race\", \"Diabetic\", \"PhysicalActivity\",\n    - \"GenHealth\", \"Asthma\", \"KidneyDisease\", \"SkinCancer\"\n  - LabelEncoder\n    - \"HeartDisease\"\n\n<br>\n\n- New method:\n  - if unique values <= 2\n    - LabelEncoder\n  - else\n    - OneHotEncoder\n","metadata":{}},{"cell_type":"code","source":"# copied original dataframe to new dataframe for new encoding methods\ndf_enc_mix = df_org.copy()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Define a LabelEncoder\nle = LabelEncoder()\n\n# Get a list of categorical column names\ncategorical_cols = df_enc_mix.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# mix of label encoding and one-hot encoding\nfor col in categorical_cols:\n    if len(df_enc_mix[col].unique()) <= 2:\n        # label encode binary variables\n        df_enc_mix[col] = le.fit_transform(df_enc_mix[col])\n    else:\n        # one-hot encode non-binary variables\n        df_enc_mix = pd.get_dummies(df_enc_mix, columns=[col])\n\ndf_enc_mix.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.1.1 Listing the unique values of each column after encoding\nHere we have all possible values on each feature, so please take a look at an example.\n\n- Original: `Diabetic : ['Yes', 'No', 'No-borderline-diabetes', 'Yes-during pregnancy']`\n- Encoded: \n  - `Diabetic_Yes : [0, 1]`\n  - `Diabetic_No : [0, 1]`\n  - `Diabetic_No-borderline-diabetes : [0, 1]`\n  - `Diabetic_Yes-during pregnancy : [0, 1]`","metadata":{"papermill":{"duration":0.039294,"end_time":"2023-05-13T04:40:37.313783","exception":false,"start_time":"2023-05-13T04:40:37.274489","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# iterate over the list to print all unique values of each column in the dataframe\nfor column in list(df.columns.values):\n    print(column, ':', str(df[column].unique()))","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:37.398164Z","iopub.status.busy":"2023-05-13T04:40:37.397169Z","iopub.status.idle":"2023-05-13T04:40:37.512954Z","shell.execute_reply":"2023-05-13T04:40:37.511434Z"},"papermill":{"duration":0.163225,"end_time":"2023-05-13T04:40:37.516761","exception":false,"start_time":"2023-05-13T04:40:37.353536","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# iterate over the list to print all unique values of each column in the dataframe\nfor column in list(df_enc_mix.columns.values):\n    print(column, ':', str(df_enc_mix[column].unique()))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2 Dataset Splitting\nHere we will show how to split a dataset into Train and Test sets. \n- The train-test split is used to estimate the performance of machine learning algorithms that are applicable for prediction-based Algorithms/Applications. \n- By default, the Test set is split into 20% of actual data and the training set is split into 80% of the actual data.","metadata":{"papermill":{"duration":0.038874,"end_time":"2023-05-13T04:40:37.593964","exception":false,"start_time":"2023-05-13T04:40:37.55509","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the dataset into train and test set with 80% and 20% respectively\ntrain_data, test_data = train_test_split(df, train_size=0.80) \ntrain_data.shape, test_data.shape","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:37.676234Z","iopub.status.busy":"2023-05-13T04:40:37.675638Z","iopub.status.idle":"2023-05-13T04:40:37.874947Z","shell.execute_reply":"2023-05-13T04:40:37.873211Z"},"papermill":{"duration":0.244075,"end_time":"2023-05-13T04:40:37.878239","exception":false,"start_time":"2023-05-13T04:40:37.634164","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the dataset into train and test set for NEW encoding method\ntrain_data_label, test_data_label = train_test_split(df_enc_mix, train_size=0.80)\ntrain_data_label.shape, test_data_label.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We need to split a dataset into train and test sets to evaluate how well our machine learning model performs. The train set is used to fit the model, and the statistics of the train set are known. The second set is called the test data set, this set is solely used for predictions.","metadata":{"papermill":{"duration":0.03883,"end_time":"2023-05-13T04:40:37.956756","exception":false,"start_time":"2023-05-13T04:40:37.917926","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# split the train and test set into features and labels\nX_train= train_data.drop('HeartDisease', axis=1)\ny_train= train_data['HeartDisease']\nprint(X_train.shape, y_train.shape)\n\nX_test= test_data.drop('HeartDisease', axis=1)\ny_test= test_data['HeartDisease']\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:38.040675Z","iopub.status.busy":"2023-05-13T04:40:38.040095Z","iopub.status.idle":"2023-05-13T04:40:38.072647Z","shell.execute_reply":"2023-05-13T04:40:38.071141Z"},"papermill":{"duration":0.079633,"end_time":"2023-05-13T04:40:38.076078","exception":false,"start_time":"2023-05-13T04:40:37.996445","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the train and test set for the NEW encoding method\nX_train_new= train_data_label.drop('HeartDisease', axis=1)\ny_train_new= train_data_label['HeartDisease']\nprint(X_train_new.shape, y_train_new.shape)\n\nX_test_new= test_data_label.drop('HeartDisease', axis=1)\ny_test_new= test_data_label['HeartDisease']\nprint(X_test_new.shape, y_test_new.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.3 Feature Scaling\nFeature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing. ","metadata":{"papermill":{"duration":0.041277,"end_time":"2023-05-13T04:40:38.255995","exception":false,"start_time":"2023-05-13T04:40:38.214718","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n# there is no data-leakage because we are using \n# information of train in test not test in train\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)\n\n# for NEW encoding method\nX_train_new=sc.fit_transform(X_train_new)\nX_test_new=sc.transform(X_test_new)","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:38.341913Z","iopub.status.busy":"2023-05-13T04:40:38.34142Z","iopub.status.idle":"2023-05-13T04:40:38.774364Z","shell.execute_reply":"2023-05-13T04:40:38.77263Z"},"papermill":{"duration":0.479258,"end_time":"2023-05-13T04:40:38.778868","exception":false,"start_time":"2023-05-13T04:40:38.29961","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train after scaling\nX_train","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:38.860244Z","iopub.status.busy":"2023-05-13T04:40:38.859388Z","iopub.status.idle":"2023-05-13T04:40:38.868584Z","shell.execute_reply":"2023-05-13T04:40:38.867209Z"},"papermill":{"duration":0.053342,"end_time":"2023-05-13T04:40:38.871729","exception":false,"start_time":"2023-05-13T04:40:38.818387","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# X_train_new after scaling\nX_train_new","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.4 Handling Imbalanced Data\nBalanced vs Imbalanced Dataset :\n\n- Balanced Dataset: In a Balanced dataset, there is approximately equal distribution of classes in the target column.\n- Imbalanced Dataset: In an Imbalanced dataset, there is a highly unequal distribution of classes in the target column.\n\nLet's take a look at our target which is 'HeartDisease':","metadata":{"papermill":{"duration":0.038533,"end_time":"2023-05-13T04:40:38.9496","exception":false,"start_time":"2023-05-13T04:40:38.911067","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df['HeartDisease'].value_counts()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:39.032347Z","iopub.status.busy":"2023-05-13T04:40:39.03141Z","iopub.status.idle":"2023-05-13T04:40:39.045703Z","shell.execute_reply":"2023-05-13T04:40:39.044278Z"},"papermill":{"duration":0.059648,"end_time":"2023-05-13T04:40:39.048565","exception":false,"start_time":"2023-05-13T04:40:38.988917","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_enc_mix['HeartDisease'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we shown in the code above, there are 292k 'No' and 27k 'Yes'then it represents an Imbalanced dataset as there is highly unequal distribution of the two classes.\n<br>\nProblem with Imbalanced dataset:\n\n- Algorithms may get biased towards the majority class and thus tend to predict output as the majority class.\n- Minority class observations look like noise to the model and are ignored by the model.\n- Imbalanced dataset gives misleading accuracy score.\n<br>\nTechniques to deal with Imbalanced dataset :\n\n- Under Sampling\n  - In this technique, we reduce the sample size of Majority class and try to match it with the sample size of Minority Class.\n    - For example, take an imbalanced training dataset with 1000 records.\n    - Before Under Sampling :\n      - Target class 'Yes' = 900\n      - Target class 'No' = 100\n    - After Under Sampling :\n      - Target class 'Yes' = 100\n      - Target class 'No' = 100\n    - Now, both classes have the same sample size.\n\n- Over Sampling\n  - In this technique, we increase the sample size of Minority class by replication and try to match it with the sample size of Majority Class.\n    - For example, Let’s take the same imbalanced training dataset with 1000 records.\n    - Before Under Sampling :\n      - Target class 'Yes' = 900\n      - Target class 'No' = 100\n    - After Under Sampling :\n      - Target class 'Yes' = 900\n      - Target class 'No' = 900\n    - Now, both classes have the same sample size.","metadata":{"papermill":{"duration":0.040539,"end_time":"2023-05-13T04:40:39.127678","exception":false,"start_time":"2023-05-13T04:40:39.087139","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"We will use both methods in our project, so we used 'SMOTE' for over_sampling and 'NearMiss' for under_sampling.","metadata":{"papermill":{"duration":0.038616,"end_time":"2023-05-13T04:40:39.20551","exception":false,"start_time":"2023-05-13T04:40:39.166894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# import SMOTE from imblearn.over_sampling\nfrom imblearn.over_sampling import SMOTE","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:39.289601Z","iopub.status.busy":"2023-05-13T04:40:39.288697Z","iopub.status.idle":"2023-05-13T04:40:39.935472Z","shell.execute_reply":"2023-05-13T04:40:39.934068Z"},"papermill":{"duration":0.694053,"end_time":"2023-05-13T04:40:39.938762","exception":false,"start_time":"2023-05-13T04:40:39.244709","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import NearMiss from imblearn.under_sampling\nfrom imblearn.under_sampling import NearMiss","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:40.023692Z","iopub.status.busy":"2023-05-13T04:40:40.022288Z","iopub.status.idle":"2023-05-13T04:40:40.029285Z","shell.execute_reply":"2023-05-13T04:40:40.028158Z"},"papermill":{"duration":0.052829,"end_time":"2023-05-13T04:40:40.032373","exception":false,"start_time":"2023-05-13T04:40:39.979544","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Counter is a collection where elements are stored as \n# dictionary keys and their counts are stored as dictionary values.\nfrom collections import Counter","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:40.118754Z","iopub.status.busy":"2023-05-13T04:40:40.117557Z","iopub.status.idle":"2023-05-13T04:40:40.124419Z","shell.execute_reply":"2023-05-13T04:40:40.122985Z"},"papermill":{"duration":0.054731,"end_time":"2023-05-13T04:40:40.127495","exception":false,"start_time":"2023-05-13T04:40:40.072764","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# balance the dataset using SMOTE (Synthetic Minority Oversampling Technique)\nsmote = SMOTE(sampling_strategy='minority')\nX_train_smote , y_train_smote = smote.fit_resample(X_train,y_train)\n\nprint('Original: {}'.format(Counter(y_train))) \nprint('   SMOTE: {}'.format(Counter(y_train_smote))) ","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:40.211115Z","iopub.status.busy":"2023-05-13T04:40:40.210589Z","iopub.status.idle":"2023-05-13T04:40:52.267191Z","shell.execute_reply":"2023-05-13T04:40:52.265257Z"},"papermill":{"duration":12.102415,"end_time":"2023-05-13T04:40:52.270542","exception":false,"start_time":"2023-05-13T04:40:40.168127","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SMOTE: NEW encoding method\nX_train_smote_new , y_train_smote_new = smote.fit_resample(X_train_new,y_train_new)\n\nprint('Original: {}'.format(Counter(y_train_new)))\nprint('  SMOTE2: {}'.format(Counter(y_train_smote_new)))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# balance the dataset using NearMiss (undersampling)\nnearmiss = NearMiss(version=3)\nX_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(X_train, y_train)\n\nprint('Original: {}'.format(Counter(y_train))) \nprint('NearMiss: {}'.format(Counter(y_train_nearmiss))) ","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:40:52.355543Z","iopub.status.busy":"2023-05-13T04:40:52.354008Z","iopub.status.idle":"2023-05-13T04:43:06.3502Z","shell.execute_reply":"2023-05-13T04:43:06.349038Z"},"papermill":{"duration":134.082058,"end_time":"2023-05-13T04:43:06.39358","exception":false,"start_time":"2023-05-13T04:40:52.311522","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NearMiss: NEW encoding method\nX_train_nearmiss_new, y_train_nearmiss_new = nearmiss.fit_resample(X_train_new, y_train_new)\n\nprint(' Original: {}'.format(Counter(y_train_new)))\nprint('NearMiss2: {}'.format(Counter(y_train_nearmiss_new)))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.5 K-Fold Cross Validation\nK-Fold CV is where a given data set is split into a K number of sections/folds where each fold is used as a testing set at some point. \n<br>\nLets take the scenario of 5-Fold cross validation(K=5). Here, the data set is split into 5 folds. In the first iteration, the first fold is used to test the model and the rest are used to train the model. In the second iteration, 2nd fold is used as the testing set while the rest serve as the training set. This process is repeated until each fold of the 5 folds have been used as the testing set.","metadata":{"papermill":{"duration":0.040107,"end_time":"2023-05-13T04:43:06.473687","exception":false,"start_time":"2023-05-13T04:43:06.43358","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# kfold cross validation\nfrom sklearn.model_selection import KFold\n\n# make a 10 fold cross validation\ncv = KFold(n_splits=10, random_state=None,shuffle=False) ","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:43:06.55916Z","iopub.status.busy":"2023-05-13T04:43:06.558185Z","iopub.status.idle":"2023-05-13T04:43:06.565397Z","shell.execute_reply":"2023-05-13T04:43:06.564123Z"},"papermill":{"duration":0.053921,"end_time":"2023-05-13T04:43:06.568619","exception":false,"start_time":"2023-05-13T04:43:06.514698","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<hr>\n\n<a id=\"title-six\"></a>\n# 6. Model Training\n\nIn this section, we compare the classification strength of AdaBoost, Random Forest, Decision Tree, KNN, Naïve Bayes, and Perceptron. After training our models, Naïve Bayes achieved the highest accuracy, whereas Perceptron reached the lowest accuracy.\n","metadata":{"papermill":{"duration":0.039425,"end_time":"2023-05-13T04:43:06.649608","exception":false,"start_time":"2023-05-13T04:43:06.610183","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# required libraries\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\n","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:43:06.732586Z","iopub.status.busy":"2023-05-13T04:43:06.731597Z","iopub.status.idle":"2023-05-13T04:43:06.749823Z","shell.execute_reply":"2023-05-13T04:43:06.748145Z"},"papermill":{"duration":0.064434,"end_time":"2023-05-13T04:43:06.753339","exception":false,"start_time":"2023-05-13T04:43:06.688905","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.1 Decision Tree","metadata":{"papermill":{"duration":0.039637,"end_time":"2023-05-13T04:43:06.832967","exception":false,"start_time":"2023-05-13T04:43:06.79333","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### 6.1.1 Decision Tree Classifier","metadata":{"papermill":{"duration":0.040327,"end_time":"2023-05-13T04:43:06.914528","exception":false,"start_time":"2023-05-13T04:43:06.874201","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# make the model and parameters\ndef dt_model():\n    model_dt = DecisionTreeClassifier()\n    params_dt = {\"criterion\":['gini','entropy'], \"max_depth\": [100], \"random_state\": [1024]}\n    model_dt_cv = GridSearchCV( model_dt, \n                                param_grid = params_dt, \n                                cv = cv, \n                                n_jobs = -1, \n                                verbose = 1 )\n    return model_dt_cv","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:43:06.998973Z","iopub.status.busy":"2023-05-13T04:43:06.997845Z","iopub.status.idle":"2023-05-13T04:44:48.918546Z","shell.execute_reply":"2023-05-13T04:44:48.916566Z"},"papermill":{"duration":102.006605,"end_time":"2023-05-13T04:44:48.961847","exception":false,"start_time":"2023-05-13T04:43:06.955242","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE\nmodel_dt_cv_smote = dt_model()\nmodel_dt_cv_smote.fit(X_train_smote ,y_train_smote)\nprint(\"Best Hyper Parameters for SMOTE: \", model_dt_cv_smote.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model for NEW encoding method - SMOTE\nmodel_dt_cv_smote_new = dt_model()\nmodel_dt_cv_smote_new.fit(X_train_smote_new ,y_train_smote_new)\nprint(\"Best Hyper Parameters for SMOTE2: \", model_dt_cv_smote_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss\nmodel_dt_cv_nearmiss = dt_model()\nmodel_dt_cv_nearmiss.fit(X_train_nearmiss ,y_train_nearmiss)\nprint(\"Best Hyper Parameters for NearMiss: \", model_dt_cv_nearmiss.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model for New encoding method - NearMiss\nmodel_dt_cv_nearmiss_new = dt_model()\nmodel_dt_cv_nearmiss_new.fit(X_train_nearmiss_new ,y_train_nearmiss_new)\nprint(\"Best Hyper Parameters for NearMiss2: \", model_dt_cv_nearmiss_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.1.2 Decision Tree Classification Report","metadata":{"papermill":{"duration":0.039239,"end_time":"2023-05-13T04:44:49.040524","exception":false,"start_time":"2023-05-13T04:44:49.001285","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print the best score (SMOTE)\ny_pred_dt_smote = model_dt_cv_smote.predict(X_test)\nprint(\"Classification Report for SMOTE: \\n\", classification_report(y_test, y_pred_dt_smote))","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:44:49.1257Z","iopub.status.busy":"2023-05-13T04:44:49.125099Z","iopub.status.idle":"2023-05-13T04:44:49.275438Z","shell.execute_reply":"2023-05-13T04:44:49.273355Z"},"papermill":{"duration":0.197349,"end_time":"2023-05-13T04:44:49.278976","exception":false,"start_time":"2023-05-13T04:44:49.081627","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (SMOTE) New encoding method\ny_pred_dt_smote_new = model_dt_cv_smote_new.predict(X_test_new)\nprint(\"Classification Report for SMOTE2: \\n\", classification_report(y_test_new, y_pred_dt_smote_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss)\ny_pred_dt_nearmiss = model_dt_cv_nearmiss.predict(X_test)\nprint(\"Classification Report for NearMiss: \\n\", classification_report(y_test, y_pred_dt_nearmiss))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss) New encoding method\ny_pred_dt_nearmiss_new = model_dt_cv_nearmiss_new.predict(X_test_new)\nprint(\"Classification Report for NearMiss2: \\n\", classification_report(y_test_new, y_pred_dt_nearmiss_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.1.3 Decision Tree Confusion Matrix","metadata":{"papermill":{"duration":0.040784,"end_time":"2023-05-13T04:44:49.362261","exception":false,"start_time":"2023-05-13T04:44:49.321477","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ncm_dt_smote = confusion_matrix(y_test,y_pred_dt_smote) # confusion matrix\ncm_dt_nearmiss = confusion_matrix(y_test,y_pred_dt_nearmiss) # confusion matrix\ncm_dt_smote_new = confusion_matrix(y_test_new,y_pred_dt_smote_new) # confusion matrix\ncm_dt_nearmiss_new = confusion_matrix(y_test_new,y_pred_dt_nearmiss_new) # confusion matrix\n\ndef plot_confusion_matrix(ax, cm, title='Confusion matrix', cmap='viridis'):\n    sn.heatmap(cm, annot=True, linewidths=0.8, fmt='d', cmap=cmap, ax=ax)\n    ax.set_xlabel('Predicted',fontsize=16)\n    ax.set_ylabel('Truth',fontsize=16)\n    ax.set_title(title,fontsize=16)\n\nfig, axs = plt.subplots(2,2, figsize=(10,4))\n\nplot_confusion_matrix(axs[0,0], cm_dt_smote, title='Decision Tree with SMOTE')\nplot_confusion_matrix(axs[0,1], cm_dt_nearmiss, title='SMOTE + New Encoding Method')\nplot_confusion_matrix(axs[1,0], cm_dt_smote_new, title='Decision Tree with NearMiss')\nplot_confusion_matrix(axs[1,1], cm_dt_nearmiss_new, title='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:44:49.447368Z","iopub.status.busy":"2023-05-13T04:44:49.446824Z","iopub.status.idle":"2023-05-13T04:44:49.785138Z","shell.execute_reply":"2023-05-13T04:44:49.783362Z"},"papermill":{"duration":0.383925,"end_time":"2023-05-13T04:44:49.788718","exception":false,"start_time":"2023-05-13T04:44:49.404793","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.1.4 Decision Tree ROC Curve","metadata":{"papermill":{"duration":0.042529,"end_time":"2023-05-13T04:44:49.872412","exception":false,"start_time":"2023-05-13T04:44:49.829883","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\ndef plot_roc_auc(ax, model_cv, X_test, y_test, label):\n    #ROC-AUC\n    #predict Probabilities  \n    y_score_model = model_cv.predict_proba(X_test) # results are probabilities for each sample for each class\n    yes_probs = y_score_model[:,1] # retrieve the probabilities only for the class1 (yes, positve class)\n\n    # calculate the features of ROC curve\n    fpr_model, tpr_model, _ = roc_curve(y_test, yes_probs) # false positive, true posistive, threshold\n\n    # AUC\n    auc_model = auc(fpr_model, tpr_model)\n\n    # plot \"No-Skill\" on ROC Curve\n    ax.plot([0,1],[0,1], linestyle='--', label='No Skill')\n\n    # Plot the ROC Curve\n    label = f'{label} (auc={auc_model:.3f})'\n    ax.plot(fpr_model, tpr_model, marker='_', label=label, color='red')\n\n    # X-axis label\n    ax.set_xlabel(\"False Positive Rate\")\n\n    # Y-axis label\n    ax.set_ylabel(\"True Positive Rate\")\n\n    # show the legend\n    ax.legend()\n","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:44:49.957025Z","iopub.status.busy":"2023-05-13T04:44:49.956447Z","iopub.status.idle":"2023-05-13T04:44:49.96806Z","shell.execute_reply":"2023-05-13T04:44:49.966589Z"},"papermill":{"duration":0.057504,"end_time":"2023-05-13T04:44:49.971122","exception":false,"start_time":"2023-05-13T04:44:49.913618","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_roc_auc(model_dt_cv, X_test, y_test, label='Decision Tree')\n\nfig, axs = plt.subplots(2,2, figsize=(12,8))\n\nplot_roc_auc(axs[0,0], model_dt_cv_smote, X_test, y_test, label='Decision Tree with SMOTE')\nplot_roc_auc(axs[0,1], model_dt_cv_smote_new, X_test_new, y_test_new, label='SMOTE + New Encoding Method')\nplot_roc_auc(axs[1,0], model_dt_cv_nearmiss, X_test, y_test, label='Decision Tree with NearMiss')\nplot_roc_auc(axs[1,1], model_dt_cv_nearmiss_new, X_test_new, y_test_new, label='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:44:50.058265Z","iopub.status.busy":"2023-05-13T04:44:50.05773Z","iopub.status.idle":"2023-05-13T04:44:50.374783Z","shell.execute_reply":"2023-05-13T04:44:50.373339Z"},"papermill":{"duration":0.364776,"end_time":"2023-05-13T04:44:50.378039","exception":false,"start_time":"2023-05-13T04:44:50.013263","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.2 AdaBoost\n#### 6.2.1 AdaBoost Classifier","metadata":{"papermill":{"duration":0.043061,"end_time":"2023-05-13T04:44:50.464787","exception":false,"start_time":"2023-05-13T04:44:50.421726","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# make the model and parameters\ndef ada_model():\n    model_ada = AdaBoostClassifier()\n    params_ada = {'n_estimators':[50], 'learning_rate':[1]}\n    model_ada_cv = GridSearchCV(model_ada, \n                                param_grid = params_ada, \n                                cv = cv, \n                                verbose = 1)\n    return model_ada_cv","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:44:50.556282Z","iopub.status.busy":"2023-05-13T04:44:50.554679Z","iopub.status.idle":"2023-05-13T04:59:17.614757Z","shell.execute_reply":"2023-05-13T04:59:17.612973Z"},"papermill":{"duration":867.154549,"end_time":"2023-05-13T04:59:17.662393","exception":false,"start_time":"2023-05-13T04:44:50.507844","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE\nmodel_ada_cv_smote = ada_model()\nmodel_ada_cv_smote.fit(X_train_smote ,y_train_smote)\nprint(\"Best Hyper Parameters for SMOTE: \", model_ada_cv_smote.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE New encoding method\nmodel_ada_cv_smote_new = ada_model()\nmodel_ada_cv_smote_new.fit(X_train_smote_new ,y_train_smote_new)\nprint(\"Best Hyper Parameters for SMOTE2: \", model_ada_cv_smote_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss\nmodel_ada_cv_nearmiss = ada_model()\nmodel_ada_cv_nearmiss.fit(X_train_nearmiss ,y_train_nearmiss)\nprint(\"Best Hyper Parameters for NearMiss: \", model_ada_cv_nearmiss.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss New encoding method\nmodel_ada_cv_nearmiss_new = ada_model()\nmodel_ada_cv_nearmiss_new.fit(X_train_nearmiss_new ,y_train_nearmiss_new)\nprint(\"Best Hyper Parameters for NearMiss2: \", model_ada_cv_nearmiss_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.2.2 AdaBoost Classification Report","metadata":{"papermill":{"duration":0.044415,"end_time":"2023-05-13T04:59:17.751198","exception":false,"start_time":"2023-05-13T04:59:17.706783","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print the best score (SMOTE)\ny_pred_ada_smote = model_ada_cv_smote.predict(X_test)\nprint(\"Classification Report for SMOTE: \\n\", classification_report(y_test, y_pred_ada_smote))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (SMOTE New encoding method)\ny_pred_ada_smote_new = model_ada_cv_smote_new.predict(X_test_new)\nprint(\"Classification Report for SMOTE2: \\n\", classification_report(y_test_new, y_pred_ada_smote_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss)\ny_pred_ada_nearmiss = model_ada_cv_nearmiss.predict(X_test)\nprint(\"Classification Report for NearMiss: \\n\", classification_report(y_test, y_pred_ada_nearmiss))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss New encoding method)\ny_pred_ada_nearmiss_new = model_ada_cv_nearmiss_new.predict(X_test_new)\nprint(\"Classification Report for NearMiss2: \\n\", classification_report(y_test_new, y_pred_ada_nearmiss_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.2.3 Confusion Matrix","metadata":{"papermill":{"duration":0.04527,"end_time":"2023-05-13T04:59:18.8318","exception":false,"start_time":"2023-05-13T04:59:18.78653","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\ncm_ada_smote = confusion_matrix(y_test, y_pred_ada_smote)\ncm_ada_nearmiss = confusion_matrix(y_test, y_pred_ada_nearmiss)\ncm_ada_smote_new = confusion_matrix(y_test_new, y_pred_ada_smote_new)\ncm_ada_nearmiss_new = confusion_matrix(y_test_new, y_pred_ada_nearmiss_new)\n\nfig, axs = plt.subplots(2,2, figsize=(10,4))\n\nplot_confusion_matrix(axs[0,0], cm_ada_smote, title='AdaBoost with SMOTE')\nplot_confusion_matrix(axs[0,1], cm_ada_nearmiss, title='SMOTE + New Encoding Method')\nplot_confusion_matrix(axs[1,0], cm_ada_smote_new, title='AdaBoost with NearMiss')\nplot_confusion_matrix(axs[1,1], cm_ada_nearmiss_new, title='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.2.4 ROC Curve","metadata":{"papermill":{"duration":0.045206,"end_time":"2023-05-13T04:59:19.757072","exception":false,"start_time":"2023-05-13T04:59:19.711866","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# plot_roc_auc(model_ada_cv, X_test, y_test, label='AdaBoost')\n\nfig, axs = plt.subplots(2,2, figsize=(12,8))\n\nplot_roc_auc(axs[0,0], model_ada_cv_smote, X_test, y_test, label='AdaBoost with SMOTE')\nplot_roc_auc(axs[0,1], model_ada_cv_smote_new, X_test_new, y_test_new, label='SMOTE + New Encoding Method')\nplot_roc_auc(axs[1,0], model_ada_cv_nearmiss, X_test, y_test, label='AdaBoost with NearMiss')\nplot_roc_auc(axs[1,1], model_ada_cv_nearmiss_new, X_test_new, y_test_new, label='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.3 Random Forest\n#### 6.3.1 Random Forest Classifier","metadata":{"papermill":{"duration":0.047256,"end_time":"2023-05-13T04:59:21.084901","exception":false,"start_time":"2023-05-13T04:59:21.037645","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# make the model and parameters\ndef rf_model():\n    model_rf = RandomForestClassifier()\n    params_rf = {\"criterion\":['gini','entropy']}\n    model_rf_cv = GridSearchCV(model_rf, \n                            param_grid = params_rf, \n                            cv = cv, \n                            verbose = 1)\n    return model_rf_cv","metadata":{"execution":{"iopub.execute_input":"2023-05-13T04:59:21.18249Z","iopub.status.busy":"2023-05-13T04:59:21.181609Z","iopub.status.idle":"2023-05-13T05:48:45.660451Z","shell.execute_reply":"2023-05-13T05:48:45.65893Z"},"papermill":{"duration":2964.579078,"end_time":"2023-05-13T05:48:45.711118","exception":false,"start_time":"2023-05-13T04:59:21.13204","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE\nmodel_rf_cv_smote = rf_model()\nmodel_rf_cv_smote.fit(X_train_smote ,y_train_smote)\nprint(\"Best Hyper Parameters for SMOTE: \", model_rf_cv_smote.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE New encoding method\nmodel_rf_cv_smote_new = rf_model()\nmodel_rf_cv_smote_new.fit(X_train_smote_new ,y_train_smote_new)\nprint(\"Best Hyper Parameters for SMOTE2: \", model_rf_cv_smote_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss\nmodel_rf_cv_nearmiss = rf_model()\nmodel_rf_cv_nearmiss.fit(X_train_nearmiss ,y_train_nearmiss)\nprint(\"Best Hyper Parameters for NearMiss: \", model_rf_cv_nearmiss.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss New encoding method\nmodel_rf_cv_nearmiss_new = rf_model()\nmodel_rf_cv_nearmiss_new.fit(X_train_nearmiss_new ,y_train_nearmiss_new)\nprint(\"Best Hyper Parameters for NearMiss2: \", model_rf_cv_nearmiss_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.3.2 Random Forest Classification Report","metadata":{"papermill":{"duration":0.046488,"end_time":"2023-05-13T05:48:45.805907","exception":false,"start_time":"2023-05-13T05:48:45.759419","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print the best score (SMOTE)\ny_pred_rf_smote = model_rf_cv_smote.predict(X_test)\nprint(\"Classification Report for SMOTE: \\n\", classification_report(y_test, y_pred_rf_smote))","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:48:45.902473Z","iopub.status.busy":"2023-05-13T05:48:45.901971Z","iopub.status.idle":"2023-05-13T05:48:49.66357Z","shell.execute_reply":"2023-05-13T05:48:49.661936Z"},"papermill":{"duration":3.813101,"end_time":"2023-05-13T05:48:49.667168","exception":false,"start_time":"2023-05-13T05:48:45.854067","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (SMOTE New encoding method)\ny_pred_rf_smote_new = model_rf_cv_smote_new.predict(X_test_new)\nprint(\"Classification Report for SMOTE2: \\n\", classification_report(y_test_new, y_pred_rf_smote_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss)\ny_pred_rf_nearmiss = model_rf_cv_nearmiss.predict(X_test)\nprint(\"Classification Report for NearMiss: \\n\", classification_report(y_test, y_pred_rf_nearmiss))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss New encoding method)\ny_pred_rf_nearmiss_new = model_rf_cv_nearmiss_new.predict(X_test_new)\nprint(\"Classification Report for NearMiss2: \\n\", classification_report(y_test_new, y_pred_rf_nearmiss_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.3.3 Random Forest Confusion Matrix","metadata":{"papermill":{"duration":0.047811,"end_time":"2023-05-13T05:48:49.767311","exception":false,"start_time":"2023-05-13T05:48:49.7195","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\ncm_rf_smote = confusion_matrix(y_test, y_pred_rf_smote)\ncm_rf_nearmiss = confusion_matrix(y_test, y_pred_rf_nearmiss)\ncm_rf_smote_new = confusion_matrix(y_test_new, y_pred_rf_smote_new)\ncm_rf_nearmiss_new = confusion_matrix(y_test_new, y_pred_rf_nearmiss_new)\n\nfig, axs = plt.subplots(2,2, figsize=(10,4))\n\nplot_confusion_matrix(axs[0,0], cm_rf_smote, title='Random Forest with SMOTE')\nplot_confusion_matrix(axs[0,1], cm_rf_nearmiss, title='SMOTE + New Encoding Method')\nplot_confusion_matrix(axs[1,0], cm_rf_smote_new, title='Random Forest with NearMiss')\nplot_confusion_matrix(axs[1,1], cm_rf_nearmiss_new, title='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:48:49.865053Z","iopub.status.busy":"2023-05-13T05:48:49.864462Z","iopub.status.idle":"2023-05-13T05:48:50.160653Z","shell.execute_reply":"2023-05-13T05:48:50.159317Z"},"papermill":{"duration":0.348861,"end_time":"2023-05-13T05:48:50.163794","exception":false,"start_time":"2023-05-13T05:48:49.814933","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.3.4 Random Forest ROC Curve","metadata":{"papermill":{"duration":0.046466,"end_time":"2023-05-13T05:48:50.257203","exception":false,"start_time":"2023-05-13T05:48:50.210737","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# plot_roc_auc(model_rf_cv, X_test, y_test, label='Random Forest')\n\nfig, axs = plt.subplots(2,2, figsize=(12,8))\n\nplot_roc_auc(axs[0,0], model_rf_cv_smote, X_test, y_test, label='Random Forest with SMOTE')\nplot_roc_auc(axs[0,1], model_rf_cv_smote_new, X_test_new, y_test_new, label='SMOTE + New Encoding Method')\nplot_roc_auc(axs[1,0], model_rf_cv_nearmiss, X_test, y_test, label='Random Forest with NearMiss')\nplot_roc_auc(axs[1,1], model_rf_cv_nearmiss_new, X_test_new, y_test_new, label='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:48:50.356604Z","iopub.status.busy":"2023-05-13T05:48:50.356079Z","iopub.status.idle":"2023-05-13T05:48:54.205731Z","shell.execute_reply":"2023-05-13T05:48:54.204328Z"},"papermill":{"duration":3.903413,"end_time":"2023-05-13T05:48:54.208844","exception":false,"start_time":"2023-05-13T05:48:50.305431","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.4 Naïve Bayes\n#### 6.4.1 Naïve Bayes Classifier","metadata":{"papermill":{"duration":0.057273,"end_time":"2023-05-13T05:48:54.322104","exception":false,"start_time":"2023-05-13T05:48:54.264831","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# make the model and parameters\ndef nb_model():\n    model_nb = GaussianNB()\n    params_nb = {'var_smoothing': np.logspace(1,10, num=100)}\n    model_nb_cv = GridSearchCV(model_nb, \n                            param_grid = params_nb, \n                            cv = cv, \n                            verbose = 1)\n    return model_nb_cv","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:48:54.426558Z","iopub.status.busy":"2023-05-13T05:48:54.426016Z","iopub.status.idle":"2023-05-13T05:59:51.366584Z","shell.execute_reply":"2023-05-13T05:59:51.364811Z"},"papermill":{"duration":657.044986,"end_time":"2023-05-13T05:59:51.419715","exception":false,"start_time":"2023-05-13T05:48:54.374729","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE\nmodel_nb_cv_smote = nb_model()\nmodel_nb_cv_smote.fit(X_train_smote ,y_train_smote)\nprint(\"Best Hyper Parameters for SMOTE: \", model_nb_cv_smote.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE New encoding method\nmodel_nb_cv_smote_new = nb_model()\nmodel_nb_cv_smote_new.fit(X_train_smote_new ,y_train_smote_new)\nprint(\"Best Hyper Parameters for SMOTE2: \", model_nb_cv_smote_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss\nmodel_nb_cv_nearmiss = nb_model()\nmodel_nb_cv_nearmiss.fit(X_train_nearmiss ,y_train_nearmiss)\nprint(\"Best Hyper Parameters for NearMiss: \", model_nb_cv_nearmiss.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss New encoding method\nmodel_nb_cv_nearmiss_new = nb_model()\nmodel_nb_cv_nearmiss_new.fit(X_train_nearmiss_new ,y_train_nearmiss_new)\nprint(\"Best Hyper Parameters for NearMiss2: \", model_nb_cv_nearmiss_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.4.2 Naïve Bayes Classification Report","metadata":{"papermill":{"duration":0.049218,"end_time":"2023-05-13T05:59:51.518707","exception":false,"start_time":"2023-05-13T05:59:51.469489","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print the best score (SMOTE)\ny_pred_nb_smote = model_nb_cv_smote.predict(X_test)\nprint(\"Classification Report for SMOTE: \\n\", classification_report(y_test, y_pred_nb_smote))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (SMOTE New encoding method)\ny_pred_nb_smote_new = model_nb_cv_smote_new.predict(X_test_new)\nprint(\"Classification Report for SMOTE2: \\n\", classification_report(y_test_new, y_pred_nb_smote_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss)\ny_pred_nb_nearmiss = model_nb_cv_nearmiss.predict(X_test)\nprint(\"Classification Report for NearMiss: \\n\", classification_report(y_test, y_pred_nb_nearmiss))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss New encoding method)\ny_pred_nb_nearmiss_new = model_nb_cv_nearmiss_new.predict(X_test_new)\nprint(\"Classification Report for NearMiss2: \\n\", classification_report(y_test_new, y_pred_nb_nearmiss_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.4.3 Naïve Bayes Confusion Matrix","metadata":{"papermill":{"duration":0.048718,"end_time":"2023-05-13T05:59:51.883653","exception":false,"start_time":"2023-05-13T05:59:51.834935","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\ncm_nb_smote = confusion_matrix(y_test, y_pred_nb_smote)\ncm_nb_nearmiss = confusion_matrix(y_test, y_pred_nb_nearmiss)\ncm_nb_smote_new = confusion_matrix(y_test_new, y_pred_nb_smote_new)\ncm_nb_nearmiss_new = confusion_matrix(y_test_new, y_pred_nb_nearmiss_new)\n\nfig, axs = plt.subplots(2,2, figsize=(10,4))\n\nplot_confusion_matrix(axs[0,0], cm_nb_smote, title='Naive Bayes with SMOTE')\nplot_confusion_matrix(axs[0,1], cm_nb_nearmiss, title='SMOTE + New Encoding Method')\nplot_confusion_matrix(axs[1,0], cm_nb_smote_new, title='Naive Bayes with NearMiss')\nplot_confusion_matrix(axs[1,1], cm_nb_nearmiss_new, title='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:59:51.985833Z","iopub.status.busy":"2023-05-13T05:59:51.984476Z","iopub.status.idle":"2023-05-13T05:59:52.21755Z","shell.execute_reply":"2023-05-13T05:59:52.216073Z"},"papermill":{"duration":0.286713,"end_time":"2023-05-13T05:59:52.220467","exception":false,"start_time":"2023-05-13T05:59:51.933754","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.4.4 Naïve Bayes ROC Curve","metadata":{"papermill":{"duration":0.048064,"end_time":"2023-05-13T05:59:52.31761","exception":false,"start_time":"2023-05-13T05:59:52.269546","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# plot_roc_auc(model_nb_cv, X_test, y_test, label='Naive Bayes')\n\nfig, axs = plt.subplots(2,2, figsize=(12,8))\n\nplot_roc_auc(axs[0,0], model_nb_cv_smote, X_test, y_test, label='Naive Bayes with SMOTE')\nplot_roc_auc(axs[0,1], model_nb_cv_smote_new, X_test_new, y_test_new, label='SMOTE + New Encoding Method')\nplot_roc_auc(axs[1,0], model_nb_cv_nearmiss, X_test, y_test, label='Naive Bayes with NearMiss')\nplot_roc_auc(axs[1,1], model_nb_cv_nearmiss_new, X_test_new, y_test_new, label='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:59:52.416538Z","iopub.status.busy":"2023-05-13T05:59:52.416012Z","iopub.status.idle":"2023-05-13T05:59:52.713002Z","shell.execute_reply":"2023-05-13T05:59:52.711666Z"},"papermill":{"duration":0.350128,"end_time":"2023-05-13T05:59:52.715999","exception":false,"start_time":"2023-05-13T05:59:52.365871","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.5 K-Nearest Neighbors\n#### 6.5.1 KNN Classifier","metadata":{"papermill":{"duration":0.050164,"end_time":"2023-05-13T05:59:52.81566","exception":false,"start_time":"2023-05-13T05:59:52.765496","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# make the model and parameters\ndef knn_model():\n    model_knn = KNeighborsClassifier()\n    params_knn = {'algorithm':['auto'], 'n_neighbors': range(1,4)}\n    model_knn_cv = GridSearchCV(model_knn, \n                            param_grid = params_knn, \n                            cv = cv, \n                            verbose = 1)\n    return model_knn_cv","metadata":{"execution":{"iopub.execute_input":"2023-05-13T05:59:52.91828Z","iopub.status.busy":"2023-05-13T05:59:52.917402Z","iopub.status.idle":"2023-05-13T15:46:28.186767Z","shell.execute_reply":"2023-05-13T15:46:28.185062Z"},"papermill":{"duration":35195.382137,"end_time":"2023-05-13T15:46:28.247596","exception":false,"start_time":"2023-05-13T05:59:52.865459","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE\nmodel_knn_cv_smote = knn_model()\nmodel_knn_cv_smote.fit(X_train_smote ,y_train_smote)\nprint(\"Best Hyper Parameters for SMOTE: \", model_knn_cv_smote.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE New encoding method\nmodel_knn_cv_smote_new = knn_model()\nmodel_knn_cv_smote_new.fit(X_train_smote_new ,y_train_smote_new)\nprint(\"Best Hyper Parameters for SMOTE2: \", model_knn_cv_smote_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss\nmodel_knn_cv_nearmiss = knn_model()\nmodel_knn_cv_nearmiss.fit(X_train_nearmiss ,y_train_nearmiss)\nprint(\"Best Hyper Parameters for NearMiss: \", model_knn_cv_nearmiss.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss New encoding method\nmodel_knn_cv_nearmiss_new = knn_model()\nmodel_knn_cv_nearmiss_new.fit(X_train_nearmiss_new ,y_train_nearmiss_new)\nprint(\"Best Hyper Parameters for NearMiss2: \", model_knn_cv_nearmiss_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.5.2 KNN Classification Report","metadata":{"papermill":{"duration":0.056276,"end_time":"2023-05-13T15:46:28.361404","exception":false,"start_time":"2023-05-13T15:46:28.305128","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print the best score (SMOTE)\ny_pred_knn_smote = model_knn_cv_smote.predict(X_test)\nprint(\"Classification Report for SMOTE: \\n\", classification_report(y_test, y_pred_knn_smote))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (SMOTE New encoding method)\ny_pred_knn_smote_new = model_knn_cv_smote_new.predict(X_test_new)\nprint(\"Classification Report for SMOTE2: \\n\", classification_report(y_test_new, y_pred_knn_smote_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss)\ny_pred_knn_nearmiss = model_knn_cv_nearmiss.predict(X_test)\nprint(\"Classification Report for NearMiss: \\n\", classification_report(y_test, y_pred_knn_nearmiss))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss New encoding method)\ny_pred_knn_nearmiss_new = model_knn_cv_nearmiss_new.predict(X_test_new)\nprint(\"Classification Report for NearMiss2: \\n\", classification_report(y_test_new, y_pred_knn_nearmiss_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.5.3 KNN Confusion Matrix","metadata":{"papermill":{"duration":0.061526,"end_time":"2023-05-13T15:52:56.705303","exception":false,"start_time":"2023-05-13T15:52:56.643777","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\ncm_knn_smote = confusion_matrix(y_test, y_pred_knn_smote)\ncm_knn_nearmiss = confusion_matrix(y_test, y_pred_knn_nearmiss)\ncm_knn_smote_new = confusion_matrix(y_test_new, y_pred_knn_smote_new)\ncm_knn_nearmiss_new = confusion_matrix(y_test_new, y_pred_knn_nearmiss_new)\n\nfig, axs = plt.subplots(2,2, figsize=(10,4))\n\nplot_confusion_matrix(axs[0,0], cm_knn_smote, title='KNN with SMOTE')\nplot_confusion_matrix(axs[0,1], cm_knn_nearmiss, title='SMOTE + New Encoding Method')\nplot_confusion_matrix(axs[1,0], cm_knn_smote_new, title='KNN with NearMiss')\nplot_confusion_matrix(axs[1,1], cm_knn_nearmiss_new, title='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T15:52:56.825973Z","iopub.status.busy":"2023-05-13T15:52:56.825558Z","iopub.status.idle":"2023-05-13T15:52:57.117779Z","shell.execute_reply":"2023-05-13T15:52:57.116666Z"},"papermill":{"duration":0.355343,"end_time":"2023-05-13T15:52:57.121889","exception":false,"start_time":"2023-05-13T15:52:56.766546","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.5.4 KNN ROC Curve","metadata":{"papermill":{"duration":0.05796,"end_time":"2023-05-13T15:52:57.239302","exception":false,"start_time":"2023-05-13T15:52:57.181342","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# plot_roc_auc(model_knn_cv, X_test, y_test, label='KNN')\n\nfig, axs = plt.subplots(2,2, figsize=(12,8))\n\nplot_roc_auc(axs[0,0], model_knn_cv_smote, X_test, y_test, label='KNN with SMOTE')\nplot_roc_auc(axs[0,1], model_knn_cv_smote_new, X_test_new, y_test_new, label='SMOTE + New Encoding Method')\nplot_roc_auc(axs[1,0], model_knn_cv_nearmiss, X_test, y_test, label='KNN with NearMiss')\nplot_roc_auc(axs[1,1], model_knn_cv_nearmiss_new, X_test_new, y_test_new, label='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T15:52:57.360047Z","iopub.status.busy":"2023-05-13T15:52:57.359111Z","iopub.status.idle":"2023-05-13T15:59:29.066173Z","shell.execute_reply":"2023-05-13T15:59:29.064564Z"},"papermill":{"duration":391.827635,"end_time":"2023-05-13T15:59:29.126996","exception":false,"start_time":"2023-05-13T15:52:57.299361","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6.6 Perceptron\n#### 6.6.1 Perceptron Classifier","metadata":{"papermill":{"duration":0.059815,"end_time":"2023-05-13T15:59:29.247189","exception":false,"start_time":"2023-05-13T15:59:29.187374","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# make the model and parameters\ndef per_model():\n    model_per = Perceptron()\n    params_per = {'tol':[0.0001], 'random_state': [2]}\n    model_per_cv = GridSearchCV(model_per, \n                            param_grid = params_per, \n                            cv = cv, \n                            refit = True,\n                            verbose = 1)\n    return model_per_cv","metadata":{"execution":{"iopub.execute_input":"2023-05-13T15:59:29.372136Z","iopub.status.busy":"2023-05-13T15:59:29.371696Z","iopub.status.idle":"2023-05-13T15:59:46.530871Z","shell.execute_reply":"2023-05-13T15:59:46.529202Z"},"papermill":{"duration":17.225953,"end_time":"2023-05-13T15:59:46.533704","exception":false,"start_time":"2023-05-13T15:59:29.307751","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE\nmodel_per_cv_smote = per_model()\nmodel_per_cv_smote.fit(X_train_smote ,y_train_smote)\nprint(\"Best Hyper Parameters for SMOTE: \", model_per_cv_smote.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using SMOTE New encoding method\nmodel_per_cv_smote_new = per_model()\nmodel_per_cv_smote_new.fit(X_train_smote_new ,y_train_smote_new)\nprint(\"Best Hyper Parameters for SMOTE2: \", model_per_cv_smote_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss\nmodel_per_cv_nearmiss = per_model()\nmodel_per_cv_nearmiss.fit(X_train_nearmiss ,y_train_nearmiss)\nprint(\"Best Hyper Parameters for NearMiss: \", model_per_cv_nearmiss.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fit the model with the best hyperparameters using NearMiss New encoding method\nmodel_per_cv_nearmiss_new = per_model()\nmodel_per_cv_nearmiss_new.fit(X_train_nearmiss_new ,y_train_nearmiss_new)\nprint(\"Best Hyper Parameters for NearMiss2: \", model_per_cv_nearmiss_new.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.6.2 Perceptron Classification Report","metadata":{"papermill":{"duration":0.061981,"end_time":"2023-05-13T15:59:46.657815","exception":false,"start_time":"2023-05-13T15:59:46.595834","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# print the best score (SMOTE)\ny_pred_per_smote = model_per_cv_smote.predict(X_test)\nprint(\"Classification Report for SMOTE: \\n\", classification_report(y_test, y_pred_per_smote))","metadata":{"execution":{"iopub.execute_input":"2023-05-13T15:59:46.784546Z","iopub.status.busy":"2023-05-13T15:59:46.784101Z","iopub.status.idle":"2023-05-13T16:06:20.704629Z","shell.execute_reply":"2023-05-13T16:06:20.703067Z"},"papermill":{"duration":394.04316,"end_time":"2023-05-13T16:06:20.764055","exception":false,"start_time":"2023-05-13T15:59:46.720895","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (SMOTE New encoding method)\ny_pred_per_smote_new = model_per_cv_smote_new.predict(X_test_new)\nprint(\"Classification Report for SMOTE2: \\n\", classification_report(y_test_new, y_pred_per_smote_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss)\ny_pred_per_nearmiss = model_per_cv_nearmiss.predict(X_test)\nprint(\"Classification Report for NearMiss: \\n\", classification_report(y_test, y_pred_per_nearmiss))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print the best score (NearMiss New encoding method)\ny_pred_per_nearmiss_new = model_per_cv_nearmiss_new.predict(X_test_new)\nprint(\"Classification Report for NearMiss2: \\n\", classification_report(y_test_new, y_pred_per_nearmiss_new))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 6.6.3 Perceptron Confusion Matrix","metadata":{"papermill":{"duration":0.059038,"end_time":"2023-05-13T16:06:20.882626","exception":false,"start_time":"2023-05-13T16:06:20.823588","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%matplotlib inline\ncm_per_smote = confusion_matrix(y_test, y_pred_per_smote)\ncm_per_nearmiss = confusion_matrix(y_test, y_pred_per_nearmiss)\ncm_per_smote_new = confusion_matrix(y_test_new, y_pred_per_smote_new)\ncm_per_nearmiss_new = confusion_matrix(y_test_new, y_pred_per_nearmiss_new)\n\nfig, axs = plt.subplots(2,2, figsize=(10,4))\n\nplot_confusion_matrix(axs[0,0], cm_per_smote, title='Perceptron with SMOTE')\nplot_confusion_matrix(axs[0,1], cm_per_nearmiss, title='SMOTE + New Encoding Method')\nplot_confusion_matrix(axs[1,0], cm_per_smote_new, title='Perceptron with NearMiss')\nplot_confusion_matrix(axs[1,1], cm_per_nearmiss_new, title='NearMiss + New Encoding Method')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2023-05-13T16:06:21.00515Z","iopub.status.busy":"2023-05-13T16:06:21.004446Z","iopub.status.idle":"2023-05-13T16:06:21.29304Z","shell.execute_reply":"2023-05-13T16:06:21.291718Z"},"papermill":{"duration":0.353629,"end_time":"2023-05-13T16:06:21.295982","exception":false,"start_time":"2023-05-13T16:06:20.942353","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}